{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72156a4-891d-4830-9f98-0a0626e756d4",
   "metadata": {},
   "source": [
    "# Amazon Review Data Pipeline\n",
    "\n",
    "The goal of this jupyter notebook is to:\n",
    "1. Load Amazon Review data acquired from [https://nijianmo.github.io/amazon/index.html](https://nijianmo.github.io/amazon/index.html)\n",
    "2. Turn it into a Spark RDD\n",
    "3. Create a Bag-of-Words with:\n",
    "    - Single words\n",
    "    - 1-grams and 2-grams\n",
    "4. Visualize the frequency of these words occurences in the Review data\n",
    "5. Create a LabeledPoint RDD for each review and save it as a libsvm file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a92afc-eb81-4451-84ad-3208159a4606",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "The data cannot be read directly from it's gzipped format into a Spark DataFrame. It produces Schema errors because there are a few naming collisions in the Json data.\n",
    "\n",
    "Instead, the data must be unzipped and a few of its errors must be correct before being loaded.\n",
    "\n",
    "The following script will achieve that. Specifically, it will ensure consistent casing for the field names 'style' and 'styleName'.\n",
    "\n",
    "Note: it is possible that both `gzip` and `sed` will need to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a35d6ebd-cbd5-4695-8b93-c259f87ab19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "\n",
    "# # Decompress Gzip\n",
    "# gzip -dk $PWD/data/Musical_Instruments_5.json.gz\n",
    "\n",
    "# # # Replace problematic field names\n",
    "# sed -i \"s:style Name:styleName:\" $PWD/data/Musical_Instruments_5.json\n",
    "# sed -i \"s:style name:styleName:\" $PWD/data/Musical_Instruments_5.json\n",
    "# sed -i \"s:Style:style:\" $PWD/data/Musical_Instruments_5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d1b6a-9aec-4aab-8a23-a4a94b397263",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5faff0f9-acca-4a77-b5a9-762ed97e3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39253a-5156-4428-b534-86d2b1945682",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "The goal of this section is to convert the Pandas dataframe from above into a Spark dataframe, to drop all columns except 'overall' (rating) and 'reviewText' and create an RDD from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0c803-8f60-45a2-810e-9f7cc0e8421d",
   "metadata": {},
   "source": [
    "#### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0bdf37a-7078-40b3-974a-8eb339c9db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 19:09:58 WARN Utils: Your hostname, Zephyrus resolves to a loopback address: 127.0.1.1; using 172.19.222.122 instead (on interface eth0)\n",
      "22/12/15 19:09:58 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/15 19:10:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/12/15 19:10:00 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[16]\") \\\n",
    "    .appName(\"data_pipeline\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4ff55-f264-43e2-a753-eb3090b03cf4",
   "metadata": {},
   "source": [
    "#### Creating Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f1c2ab7-f511-4105-9de7-31417fbfc8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color Name:: string (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Configuration:: string (nullable = true)\n",
      " |    |-- Edition:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Display Length:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Model Number:: string (nullable = true)\n",
      " |    |-- Number of Items:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Platform for Display:: string (nullable = true)\n",
      " |    |-- Platform:: string (nullable = true)\n",
      " |    |-- Product Packaging:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- style:: string (nullable = true)\n",
      " |    |-- styleName:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.json('data/Musical_Instruments_5.json')\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e3a67d-c472-44e2-9f99-a85852f8febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 231392 rows of data\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {spark_df.count()} rows of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c58fe8-6b1d-4b11-acf4-3e1a625355bf",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "1. Drop all columns except `overall` and `reviewTest`\n",
    "2. Drop all rows with NA values\n",
    "3. Convert to RDD\n",
    "4. Convert to lowercase\n",
    "5. Strip non-alphabetic/space chars\n",
    "6. Split into vector of strings\n",
    "7. Remove empty reviews\n",
    "8. Simplify rating into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "796f6abc-37af-468f-94cc-8004a9f950ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method RDD.cache of PythonRDD[18] at RDD at PythonRDD.scala:53>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all columns except reviewText and overall\n",
    "df = spark_df[['overall', 'reviewText']]\n",
    "df.printSchema()\n",
    "\n",
    "# Drop all rows with NA values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert to rdd\n",
    "rdd = df.rdd\n",
    "\n",
    "# Convert to lowercase\n",
    "rdd = rdd.map(lambda x: (x[0], x[1].lower()))\n",
    "\n",
    "# Strip non-alphanumeric & space chars\n",
    "rdd = rdd.map(lambda x: (x[0], re.sub(r'[^A-Za-z ]+', '', x[1])))\n",
    "\n",
    "# Convert review to array of words\n",
    "rdd = rdd.map(lambda x: (x[0], x[1].split()))\n",
    "\n",
    "# Convert overall rating to 1 or 0\n",
    "# 4-5 -> Positive\n",
    "# 1-3 -> Negative\n",
    "rdd = rdd.map(lambda x: (1, x[1]) if x[0] in [4,5] else (0, x[1]))\n",
    "\n",
    "# Filter out empty reviews\n",
    "basic_rdd = rdd.filter(lambda x: len(x[1]) > 0)\n",
    "\n",
    "# Cache as basic_rdd\n",
    "total_rows = basic_rdd.count()\n",
    "basic_rdd.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1acde-6455-4afc-a215-e18839e971a8",
   "metadata": {},
   "source": [
    "## Creating Bags-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f52fd-e5a7-45f8-9b1f-68937f811e70",
   "metadata": {},
   "source": [
    "### Method for Filtering Stop Words\n",
    "\n",
    "For all $(k, v)$ where $v$ is a list of strings, remove all $e$ from $v$ where $e \\in \\text{stopwords}$. \n",
    "\n",
    "Stopwords sourced from [here](http://snowball.tartarus.org/algorithms/english/stop.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b49f4fb-4abb-47fa-b9b9-d28b0aef7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "with open(\"data/stopwords.txt\") as file:\n",
    "    global stopwords\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff150be-f1d6-4a73-97ee-319558f3ebd2",
   "metadata": {},
   "source": [
    "### Creating Set of OneGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b17701-b584-4c12-9eab-0a424583d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    global stopwords\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if not s in stopwords:\n",
    "            result.append(s)\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f9430d-31c6-418e-a2de-4a81bd3bbabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "positive_onegram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 1) \\\n",
    "    .map(remove_stopwords) \\\n",
    "    .flatMap(lambda x: x[1]) \\\n",
    "    .map(lambda x: (tuple([x]), 1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])\n",
    "\n",
    "negative_onegram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 0) \\\n",
    "    .map(remove_stopwords) \\\n",
    "    .flatMap(lambda x: x[1]) \\\n",
    "    .map(lambda x: (tuple([x]), 1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2739ebf6-bfb5-4a2f-9569-9a8945156510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 53160 shared Positive OneGrams in the corpus\n",
      "There are 21554 shared Negative OneGrams in the corpus\n"
     ]
    }
   ],
   "source": [
    "num_shared_pos_onegrams = positive_onegram_counts.count()\n",
    "num_shared_neg_onegrams = negative_onegram_counts.count()\n",
    "\n",
    "positive_onegram_counts.cache\n",
    "negative_onegram_counts.cache\n",
    "\n",
    "print(f\"There are {num_shared_pos_onegrams} shared Positive OneGrams in the corpus\")\n",
    "print(f\"There are {num_shared_neg_onegrams} shared Negative OneGrams in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb9accf7-615f-48c9-9005-0d7ad2e4ac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive OneGrams:\n",
      "\t(('great',), 74484)\n",
      "\t(('good',), 54966)\n",
      "\t(('guitar',), 51021)\n",
      "\t(('sound',), 47321)\n",
      "\t(('one',), 44627)\n",
      "\t(('like',), 44025)\n",
      "Negative OneGrams:\n",
      "\t(('one',), 10232)\n",
      "\t(('just',), 9416)\n",
      "\t(('like',), 9228)\n",
      "\t(('good',), 8735)\n",
      "\t(('guitar',), 8474)\n",
      "\t(('sound',), 8006)\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive OneGrams:\")\n",
    "for a in positive_onegram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)\n",
    "\n",
    "print(\"Negative OneGrams:\")\n",
    "for a in negative_onegram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f2b28-9b7f-4fe0-b42b-876cc773477b",
   "metadata": {},
   "source": [
    "Number of distinct words:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f0687-8286-499d-94f3-e2e7afa83961",
   "metadata": {},
   "source": [
    "### Creating Set of TwoGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0caaa1d2-2fb0-4a69-b5f1-426ae424141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "def ngram(x):\n",
    "    result = []\n",
    "    for i in range(len(x[1])-n+1):\n",
    "        gram = []\n",
    "        for j in range(i, i+n):\n",
    "            gram.append(x[1][j])\n",
    "        result.append(tuple(gram))\n",
    "    return (x[0], result)\n",
    "\n",
    "def ngram_remove_stopwords(x):\n",
    "    global stopwords\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if not s[0] in stopwords and not s[-1] in stopwords:\n",
    "            result.append(s)\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2ee7400-f3c8-4eef-bc38-1d603bd69b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "positive_twogram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 1) \\\n",
    "    .map(ngram) \\\n",
    "    .map(ngram_remove_stopwords) \\\n",
    "    .flatMap(lambda x: [(a, 1) for a in x[1]]) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])\n",
    "\n",
    "negative_twogram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 0) \\\n",
    "    .map(ngram) \\\n",
    "    .map(ngram_remove_stopwords) \\\n",
    "    .flatMap(lambda x: [(a, 1) for a in x[1]]) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8828a489-04a1-42b4-b7c6-36ec69586c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 245943 shared Positive TwoGrams in the corpus\n",
      "There are 59697 shared Negative TwoGrams in the corpus\n"
     ]
    }
   ],
   "source": [
    "num_shared_pos_twograms = positive_twogram_counts.count()\n",
    "num_shared_neg_twograms = negative_twogram_counts.count()\n",
    "\n",
    "positive_twogram_counts.cache\n",
    "negative_twogram_counts.cache\n",
    "\n",
    "print(f\"There are {num_shared_pos_twograms} shared Positive TwoGrams in the corpus\")\n",
    "print(f\"There are {num_shared_neg_twograms} shared Negative TwoGrams in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a2277ad-0c71-48f1-9fc6-ac4a94e86d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive TwoGrams:\n",
      "\t(('works', 'great'), 7120)\n",
      "\t(('good', 'quality'), 3826)\n",
      "\t(('great', 'product'), 3631)\n",
      "\t(('great', 'price'), 3372)\n",
      "\t(('highly', 'recommend'), 2990)\n",
      "\t(('sounds', 'great'), 2702)\n",
      "Negative TwoGrams:\n",
      "\t(('much', 'better'), 658)\n",
      "\t(('dont', 'know'), 628)\n",
      "\t(('can', 'get'), 451)\n",
      "\t(('power', 'supply'), 441)\n",
      "\t(('didnt', 'work'), 425)\n",
      "\t(('sound', 'quality'), 424)\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive TwoGrams:\")\n",
    "for a in positive_twogram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)\n",
    "\n",
    "print(\"Negative TwoGrams:\")\n",
    "for a in negative_twogram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad30ae4-5e6b-4f7b-80f7-de3871183001",
   "metadata": {},
   "source": [
    "### Determining Bag Sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a61d294d-1078-432c-814d-49e139987e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "onegrams = set(positive_onegram_counts.map(lambda x: x[0]).collect()).union(set(negative_onegram_counts.map(lambda x: x[0]).collect()))\n",
    "twograms = set(positive_twogram_counts.map(lambda x: x[0]).collect()).union(set(negative_twogram_counts.map(lambda x: x[0]).collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "86a24be4-8ec8-47bf-8c78-8e65d5ea766d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 56745 unique shared onegrams\n",
      "There are 268549 unique shared onegrams\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(onegrams)} unique shared onegrams\")\n",
    "print(f\"There are {len(twograms)} unique shared onegrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5289cc5b-dc7f-402d-9cc2-dc85453ef347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OneGram Bag Size: 5674\n",
      "TwoGram Bag Size: 26854\n"
     ]
    }
   ],
   "source": [
    "bagsize_onegram = int(len(onegrams)/10)\n",
    "bagsize_twogram = int(len(twograms)/10)\n",
    "\n",
    "print(f\"OneGram Bag Size: {bagsize_onegram}\")\n",
    "print(f\"TwoGram Bag Size: {bagsize_twogram}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9371dc7-85da-44f8-88f1-e065a6815a69",
   "metadata": {},
   "source": [
    "### Generating Bag of Top 10% of OneGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "594b6d3d-be0c-48e6-af83-8c73b84f7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos_onegrams = set(positive_onegram_counts.map(lambda x: x[0]).take(bagsize_onegram))\n",
    "top_neg_onegrams = set(negative_onegram_counts.map(lambda x: x[0]).take(bagsize_onegram))\n",
    "common_onegrams = top_pos_onegrams.intersection(top_neg_onegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "efeb6c79-3b74-44fd-9032-8e5f6ce0715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = int((bagsize_onegram - len(common_onegrams)) / 2)\n",
    "num_neg = bagsize_onegram - len(common_onegrams) - num_pos\n",
    "\n",
    "unique_pos_onegrams = set(positive_onegram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_onegrams) \\\n",
    "    .take(num_pos))\n",
    "\n",
    "unique_neg_onegrams = set(negative_onegram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_onegrams) \\\n",
    "    .take(num_neg))\n",
    "\n",
    "bag_onegram = common_onegrams.union(unique_neg_onegrams).union(unique_pos_onegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "8acb4798-4e82-4053-800f-d4bdd6505580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4890 common OneGrams\n",
      "We will include 392 unique positive OneGrams\n",
      "We will include 392 unique negative OneGrams\n",
      "There are 5674 OneGrams in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(common_onegrams)} common OneGrams\")\n",
    "print(f\"We will include {len(unique_pos_onegrams)} unique positive OneGrams\")\n",
    "print(f\"We will include {len(unique_neg_onegrams)} unique negative OneGrams\")\n",
    "print(f\"There are {len(bag_onegram)} OneGrams in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d856f1-d215-4aeb-aa77-fc17bbabaeb9",
   "metadata": {},
   "source": [
    "### Generating Bag of Top 10% of TwoGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "38b28893-36de-4846-a5d8-9723969be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pos_twograms = set(positive_twogram_counts.map(lambda x: x[0]).take(bagsize_twogram))\n",
    "top_neg_twograms = set(negative_twogram_counts.map(lambda x: x[0]).take(bagsize_twogram))\n",
    "common_twograms = top_pos_twograms.intersection(top_neg_twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "d204e997-186f-4138-b0ee-e52ff3e22eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos_two = int((bagsize_twogram - len(common_twograms)) / 2)\n",
    "num_neg_two = bagsize_twogram - len(common_twograms) - num_pos_two\n",
    "\n",
    "unique_pos_twograms = set(positive_twogram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_twograms) \\\n",
    "    .take(num_pos_two))\n",
    "\n",
    "unique_neg_twograms = set(negative_twogram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_twograms) \\\n",
    "    .take(num_neg_two))\n",
    "\n",
    "bag_twogram = common_twograms.union(unique_neg_twograms).union(unique_pos_twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9f5c9127-0763-4fc8-9770-0c19094c117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13879 common TwoGrams\n",
      "We will include 6487 unique Positive TwoGrams\n",
      "We will include 6488 unique Negative TwoGrams\n",
      "There are 26854 NGrams in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(common_twograms)} common TwoGrams\")\n",
    "print(f\"We will include {len(unique_pos_twograms)} unique Positive TwoGrams\")\n",
    "print(f\"We will include {len(unique_neg_twograms)} unique Negative TwoGrams\")\n",
    "# print(f\"We will include {len(bag_3k)} OneGrams\")\n",
    "print(f\"There are {len(bag_twogram)} NGrams in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4ed51-8b5d-497e-bed3-6ecb7775600e",
   "metadata": {},
   "source": [
    "## Create Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6313d-009a-4fb4-bbd5-f080934b12a5",
   "metadata": {},
   "source": [
    "### Generate OneGram RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "140025ac-391c-48a7-8b01-99354e9d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "onegram_tuples_rdd = basic_rdd \\\n",
    "    .map(lambda x: (x[0], [tuple([s]) for s in x[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3cafe-488c-4258-ae57-54a2e07482cb",
   "metadata": {},
   "source": [
    "### Generate TwoGram RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bd59ca2b-eb69-4bc0-8d85-f4ff0d1047e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "def two_grams(x):\n",
    "    result = []\n",
    "    # for s in x[1]:\n",
    "    #     result.append(tuple([s])) # Get all OneGrams\n",
    "    for i in range(len(x[1])-n+1):\n",
    "        gram = []\n",
    "        for j in range(i, i+n):\n",
    "            gram.append(x[1][j])\n",
    "        result.append(tuple(gram)) # Get all TwoGrams\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "867a83c4-e83a-4529-a1c1-c8b85eedb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twogram_tuples_rdd = basic_rdd.map(two_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c223c1-a93d-4d0c-bc04-8a98d2f90f90",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "f9c19b5d-5785-4ff6-ad78-a0c32491e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_filter(x, bag):\n",
    "    global count\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if s in bag:\n",
    "            result.append(s)\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "95c0569a-f4b9-414f-8c86-67c525886eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "data_onegram = onegram_tuples_rdd \\\n",
    "    .map(lambda x: bag_filter(x, bag_onegram)) \\\n",
    "    .filter(lambda x: len(x[1]) > 0) # Ensure there is at least one feature\n",
    "\n",
    "data_twogram = twogram_tuples_rdd \\\n",
    "    .map(lambda x: bag_filter(x, bag_twogram)) \\\n",
    "    .filter(lambda x: len(x[1]) > 0) # Ensure there is at least one feature\n",
    "\n",
    "num_remaining_one = data_onegram.count()\n",
    "num_remaining_two = data_twogram.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "37d1e155-d1b6-43d5-8db0-f83d8ab1d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.53% of rows remain after Bag_OneGram filter\n",
      "84.54% of rows remain after Bag_TwoGram filter\n"
     ]
    }
   ],
   "source": [
    "print(f\"{num_remaining_one/total_rows*100:.2f}% of rows remain after Bag_OneGram filter\")\n",
    "print(f\"{num_remaining_two/total_rows*100:.2f}% of rows remain after Bag_TwoGram filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf80d2-3c3b-4f1f-b0a9-bf42db2168d8",
   "metadata": {},
   "source": [
    "## Prepare Data for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dc175-df02-49ca-b28a-ac7347d60131",
   "metadata": {},
   "source": [
    "### Convert to RDD\\<LabeledPoint\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "7770e6d4-eede-4c68-a848-4069478f8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "91fa812d-e55a-4377-af19-dfeca9917be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lp_rdd(x, bag):\n",
    "    feature_ids = []\n",
    "    feature_vals = []\n",
    "    for id,val in enumerate(bag):\n",
    "        if val in x[1]:\n",
    "            feature_ids.append(id)\n",
    "            feature_vals.append(1)\n",
    "    return LabeledPoint(x[0], SparseVector(len(bag), feature_ids, feature_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "91d7e834-5dcd-468e-a62d-051f8fb9bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "5dc6ca5c-3d23-4b58-8bd3-b8e3e260920b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "data_A = data_onegram.map(lambda x: convert_to_lp_rdd(x, bag_onegram))\n",
    "data_B = data_twogram.map(lambda x: convert_to_lp_rdd(x, bag_twogram))\n",
    "\n",
    "if not os.path.isdir(\"data/data_A\"):\n",
    "    MLUtils.saveAsLibSVMFile(data_A, \"data/data_A\")\n",
    "if not os.path.isdir(\"data/data_B\"):\n",
    "    MLUtils.saveAsLibSVMFile(data_B, \"data/data_B\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
