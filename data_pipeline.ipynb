{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72156a4-891d-4830-9f98-0a0626e756d4",
   "metadata": {},
   "source": [
    "# Amazon Review Data Pipeline\n",
    "\n",
    "The goal of this jupyter notebook is to:\n",
    "1. Load Amazon Review data acquired from [https://nijianmo.github.io/amazon/index.html](https://nijianmo.github.io/amazon/index.html)\n",
    "2. Turn it into a Spark RDD\n",
    "3. Create a Bag-of-Words with:\n",
    "    - Single words\n",
    "    - 1-grams and 2-grams\n",
    "4. Visualize the frequency of these words occurences in the Review data\n",
    "5. Create a LabeledPoint RDD for each review and save it as a libsvm file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a92afc-eb81-4451-84ad-3208159a4606",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "The data cannot be read directly from it's gzipped format into a Spark DataFrame. It produces Schema errors because there are a few naming collisions in the Json data.\n",
    "\n",
    "Instead, the data must be unzipped and a few of its errors must be correct before being loaded.\n",
    "\n",
    "The following script will achieve that. Specifically, it will ensure consistent casing for the field names 'style' and 'styleName'.\n",
    "\n",
    "Note: it is possible that both `gzip` and `sed` will need to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a35d6ebd-cbd5-4695-8b93-c259f87ab19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: /home/steph/proj/550/project/data/Musical_Instruments_5.json already exists;\tnot overwritten\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Decompress Gzip\n",
    "gzip -dk $PWD/data/Musical_Instruments_5.json.gz\n",
    "\n",
    "# # Replace problematic field names\n",
    "sed -i \"s:style Name:styleName:\" $PWD/data/Musical_Instruments_5.json\n",
    "sed -i \"s:style name:styleName:\" $PWD/data/Musical_Instruments_5.json\n",
    "sed -i \"s:Style:style:\" $PWD/data/Musical_Instruments_5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d1b6a-9aec-4aab-8a23-a4a94b397263",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "5faff0f9-acca-4a77-b5a9-762ed97e3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.mllib.util import MLUtils\n",
    "import os\n",
    "import gzip\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39253a-5156-4428-b534-86d2b1945682",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "The goal of this section is to convert the Pandas dataframe from above into a Spark dataframe, to drop all columns except 'overall' (rating) and 'reviewText' and create an RDD from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0c803-8f60-45a2-810e-9f7cc0e8421d",
   "metadata": {},
   "source": [
    "#### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c0bdf37a-7078-40b3-974a-8eb339c9db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 17:53:05 WARN Utils: Your hostname, Zephyrus resolves to a loopback address: 127.0.1.1; using 172.17.20.254 instead (on interface eth0)\n",
      "22/12/14 17:53:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/14 17:53:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[16]\") \\\n",
    "    .appName(\"data_pipeline\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4ff55-f264-43e2-a753-eb3090b03cf4",
   "metadata": {},
   "source": [
    "#### Creating Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1c2ab7-f511-4105-9de7-31417fbfc8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                        (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color Name:: string (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Configuration:: string (nullable = true)\n",
      " |    |-- Edition:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Display Length:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Model Number:: string (nullable = true)\n",
      " |    |-- Number of Items:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Platform for Display:: string (nullable = true)\n",
      " |    |-- Platform:: string (nullable = true)\n",
      " |    |-- Product Packaging:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- style:: string (nullable = true)\n",
      " |    |-- styleName:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.json('data/Musical_Instruments_5.json')\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80e3a67d-c472-44e2-9f99-a85852f8febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 231392 rows of data\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {spark_df.count()} rows of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c58fe8-6b1d-4b11-acf4-3e1a625355bf",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "1. Drop all columns except `overall` and `reviewTest`\n",
    "2. Drop all rows with NA values\n",
    "3. Convert to RDD\n",
    "4. Convert to lowercase\n",
    "5. Strip non-alphabetic/space chars\n",
    "6. Split into vector of strings\n",
    "7. Remove empty reviews\n",
    "8. Simplify rating into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "796f6abc-37af-468f-94cc-8004a9f950ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method RDD.cache of PythonRDD[18] at RDD at PythonRDD.scala:53>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all columns except reviewText and overall\n",
    "df = spark_df[['overall', 'reviewText']]\n",
    "df.printSchema()\n",
    "\n",
    "# Drop all rows with NA values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert to rdd\n",
    "rdd = df.rdd\n",
    "\n",
    "# Convert to lowercase\n",
    "rdd = rdd.map(lambda x: (x[0], x[1].lower()))\n",
    "\n",
    "# Strip non-alphanumeric & space chars\n",
    "rdd = rdd.map(lambda x: (x[0], re.sub(r'[^A-Za-z ]+', '', x[1])))\n",
    "\n",
    "# Convert review to array of words\n",
    "rdd = rdd.map(lambda x: (x[0], x[1].split()))\n",
    "\n",
    "# Convert overall rating to 1 or 0\n",
    "# 4-5 -> Positive\n",
    "# 1-3 -> Negative\n",
    "rdd = rdd.map(lambda x: (1, x[1]) if x[0] in [4,5] else (0, x[1]))\n",
    "\n",
    "# Filter out empty reviews\n",
    "basic_rdd = rdd.filter(lambda x: len(x[1]) > 0)\n",
    "\n",
    "# Cache as basic_rdd\n",
    "total_rows = basic_rdd.count()\n",
    "basic_rdd.cache"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1acde-6455-4afc-a215-e18839e971a8",
   "metadata": {},
   "source": [
    "## Creating Bags-of-Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f52fd-e5a7-45f8-9b1f-68937f811e70",
   "metadata": {},
   "source": [
    "### Method for Filtering Stop Words\n",
    "\n",
    "For all $(k, v)$ where $v$ is a list of strings, remove all $e$ from $v$ where $e \\in \\text{stopwords}$. \n",
    "\n",
    "Stopwords sourced from [here](http://snowball.tartarus.org/algorithms/english/stop.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "7b49f4fb-4abb-47fa-b9b9-d28b0aef7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "with open(\"data/stopwords.txt\") as file:\n",
    "    global stopwords\n",
    "    stopwords = file.read().splitlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff150be-f1d6-4a73-97ee-319558f3ebd2",
   "metadata": {},
   "source": [
    "### Creating Positive OneGram Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "15b17701-b584-4c12-9eab-0a424583d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords(x):\n",
    "    global stopwords\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if not s in stopwords:\n",
    "            result.append(s)\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "02f9430d-31c6-418e-a2de-4a81bd3bbabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "positive_onegram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 1) \\\n",
    "    .map(remove_stopwords) \\\n",
    "    .flatMap(lambda x: x[1]) \\\n",
    "    .map(lambda x: (tuple([x]), 1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])\n",
    "\n",
    "negative_onegram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 0) \\\n",
    "    .map(remove_stopwords) \\\n",
    "    .flatMap(lambda x: x[1]) \\\n",
    "    .map(lambda x: (tuple([x]), 1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "2739ebf6-bfb5-4a2f-9569-9a8945156510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 53160 shared Positive OneGrams in the corpus\n",
      "There are 21554 shared Negative OneGrams in the corpus\n"
     ]
    }
   ],
   "source": [
    "num_shared_pos_onegrams = positive_onegram_counts.count()\n",
    "num_shared_neg_onegrams = negative_onegram_counts.count()\n",
    "\n",
    "positive_onegram_counts.cache\n",
    "negative_onegram_counts.cache\n",
    "\n",
    "print(f\"There are {num_shared_pos_onegrams} shared Positive OneGrams in the corpus\")\n",
    "print(f\"There are {num_shared_neg_onegrams} shared Negative OneGrams in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "cb9accf7-615f-48c9-9005-0d7ad2e4ac9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive OneGrams:\n",
      "\t(('great',), 74484)\n",
      "\t(('good',), 54966)\n",
      "\t(('guitar',), 51021)\n",
      "\t(('sound',), 47321)\n",
      "\t(('one',), 44627)\n",
      "\t(('like',), 44025)\n",
      "Negative OneGrams:\n",
      "\t(('one',), 10232)\n",
      "\t(('just',), 9416)\n",
      "\t(('like',), 9228)\n",
      "\t(('good',), 8735)\n",
      "\t(('guitar',), 8474)\n",
      "\t(('sound',), 8006)\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive OneGrams:\")\n",
    "for a in positive_onegram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)\n",
    "\n",
    "print(\"Negative OneGrams:\")\n",
    "for a in negative_onegram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "012f2b28-9b7f-4fe0-b42b-876cc773477b",
   "metadata": {},
   "source": [
    "Number of distinct words:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f0687-8286-499d-94f3-e2e7afa83961",
   "metadata": {},
   "source": [
    "### Creating ThreeGram Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "0caaa1d2-2fb0-4a69-b5f1-426ae424141f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "\n",
    "def ngram(x):\n",
    "    result = []\n",
    "    for i in range(len(x[1])-n+1):\n",
    "        gram = []\n",
    "        for j in range(i, i+n):\n",
    "            gram.append(x[1][j])\n",
    "        result.append(tuple(gram))\n",
    "    return (x[0], result)\n",
    "\n",
    "def ngram_remove_stopwords(x):\n",
    "    global stopwords\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if not s[0] in stopwords and not s[-1] in stopwords:\n",
    "            result.append(s)\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "c2ee7400-f3c8-4eef-bc38-1d603bd69b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "positive_twogram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 1) \\\n",
    "    .map(ngram) \\\n",
    "    .map(ngram_remove_stopwords) \\\n",
    "    .flatMap(lambda x: [(a, 1) for a in x[1]]) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])\n",
    "\n",
    "negative_twogram_counts = basic_rdd \\\n",
    "    .filter(lambda x: x[0] == 0) \\\n",
    "    .map(ngram) \\\n",
    "    .map(ngram_remove_stopwords) \\\n",
    "    .flatMap(lambda x: [(a, 1) for a in x[1]]) \\\n",
    "    .reduceByKey(lambda a,b: a+b) \\\n",
    "    .filter(lambda x: x[1] > 1) \\\n",
    "    .sortBy(lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "8828a489-04a1-42b4-b7c6-36ec69586c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 245943 shared Positive TwoGrams in the corpus\n",
      "There are 59697 shared Negative TwoGrams in the corpus\n"
     ]
    }
   ],
   "source": [
    "num_shared_pos_twograms = positive_twogram_counts.count()\n",
    "num_shared_neg_twograms = negative_twogram_counts.count()\n",
    "\n",
    "positive_twogram_counts.cache\n",
    "negative_twogram_counts.cache\n",
    "\n",
    "print(f\"There are {num_shared_pos_twograms} shared Positive TwoGrams in the corpus\")\n",
    "print(f\"There are {num_shared_neg_twograms} shared Negative TwoGrams in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2a2277ad-0c71-48f1-9fc6-ac4a94e86d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive TwoGrams:\n",
      "\t(('works', 'great'), 7120)\n",
      "\t(('good', 'quality'), 3826)\n",
      "\t(('great', 'product'), 3631)\n",
      "\t(('great', 'price'), 3372)\n",
      "\t(('highly', 'recommend'), 2990)\n",
      "\t(('sounds', 'great'), 2702)\n",
      "Negative TwoGrams:\n",
      "\t(('much', 'better'), 658)\n",
      "\t(('dont', 'know'), 628)\n",
      "\t(('can', 'get'), 451)\n",
      "\t(('power', 'supply'), 441)\n",
      "\t(('didnt', 'work'), 425)\n",
      "\t(('sound', 'quality'), 424)\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive TwoGrams:\")\n",
    "for a in positive_twogram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)\n",
    "\n",
    "print(\"Negative TwoGrams:\")\n",
    "for a in negative_twogram_counts.take(6):\n",
    "    print('\\t', end=\"\")\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9371dc7-85da-44f8-88f1-e065a6815a69",
   "metadata": {},
   "source": [
    "### Generating Bag of 3000 OneGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "594b6d3d-be0c-48e6-af83-8c73b84f7878",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3k_pos_onegrams = set(positive_onegram_counts.map(lambda x: x[0]).take(3000))\n",
    "top_3k_neg_onegrams = set(negative_onegram_counts.map(lambda x: x[0]).take(3000))\n",
    "common_3k_onegrams = top_3k_pos_onegrams.intersection(top_3k_neg_onegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "efeb6c79-3b74-44fd-9032-8e5f6ce0715f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_3k_pos = int((3000 - len(common_3k_onegrams)) / 2)\n",
    "num_3k_neg = 3000 - len(common_3k_onegrams) - num_3k_pos\n",
    "\n",
    "unique_3k_pos_onegrams = set(positive_onegram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_3k_onegrams) \\\n",
    "    .take(num_3k_pos))\n",
    "\n",
    "unique_3k_neg_onegrams = set(negative_onegram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_3k_onegrams) \\\n",
    "    .take(num_3k_neg))\n",
    "\n",
    "bag_3k = common_3k_onegrams.union(unique_3k_neg_onegrams).union(unique_3k_pos_onegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "8acb4798-4e82-4053-800f-d4bdd6505580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2630 common OneGrams\n",
      "We will include 185 unique positive OneGrams\n",
      "We will include 185 unique positive OneGrams\n",
      "There are 3000 OneGrams in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(common_3k_onegrams)} common OneGrams\")\n",
    "print(f\"We will include {len(unique_3k_pos_onegrams)} unique positive OneGrams\")\n",
    "print(f\"We will include {len(unique_3k_neg_onegrams)} unique positive OneGrams\")\n",
    "print(f\"There are {len(bag_3k)} OneGrams in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277498b2-460c-4cd1-8abc-72694cc97077",
   "metadata": {},
   "source": [
    "### Generating Bag of 5000 OneGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "cf900d3f-973c-4e3d-8ad0-04dc930d897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_5k_pos_onegrams = set(positive_onegram_counts.map(lambda x: x[0]).take(5000))\n",
    "top_5k_neg_onegrams = set(negative_onegram_counts.map(lambda x: x[0]).take(5000))\n",
    "common_5k_onegrams = top_5k_pos_onegrams.intersection(top_5k_neg_onegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "8a705ed9-5722-489b-9dc4-a28afe30ecff",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_5k_pos = int((5000 - len(common_5k_onegrams)) / 2)\n",
    "num_5k_neg = 5000 - len(common_5k_onegrams) - num_5k_pos\n",
    "\n",
    "unique_5k_pos_onegrams = set(positive_onegram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_5k_onegrams) \\\n",
    "    .take(num_5k_pos))\n",
    "\n",
    "unique_5k_neg_onegrams = set(negative_onegram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_5k_onegrams) \\\n",
    "    .take(num_5k_neg))\n",
    "\n",
    "bag_5k_a = common_5k_onegrams.union(unique_5k_neg_onegrams).union(unique_5k_pos_onegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "4673709d-0824-4ecb-a842-ef28e0670db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4330 common OneGrams\n",
      "We will include 335 unique Positive OneGrams\n",
      "We will include 335 unique Negative OneGrams\n",
      "There are 5000 OneGrams in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(common_5k_onegrams)} common OneGrams\")\n",
    "print(f\"We will include {len(unique_5k_pos_onegrams)} unique Positive OneGrams\")\n",
    "print(f\"We will include {len(unique_5k_neg_onegrams)} unique Negative OneGrams\")\n",
    "print(f\"There are {len(bag_5k_a)} OneGrams in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d856f1-d215-4aeb-aa77-fc17bbabaeb9",
   "metadata": {},
   "source": [
    "### Generating Bag of 3000 OneGrams + 2000 TwoGrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "38b28893-36de-4846-a5d8-9723969be924",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_2k_pos_twograms = set(positive_twogram_counts.map(lambda x: x[0]).take(2000))\n",
    "top_2k_neg_twograms = set(negative_twogram_counts.map(lambda x: x[0]).take(2000))\n",
    "common_2k_twograms = top_2k_pos_twograms.intersection(top_2k_neg_twograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "d204e997-186f-4138-b0ee-e52ff3e22eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_2k_pos = int((2000 - len(common_2k_twograms)) / 2)\n",
    "num_2k_neg = 2000 - len(common_2k_twograms) - num_2k_pos\n",
    "\n",
    "unique_2k_pos_twograms = set(positive_twogram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_2k_twograms) \\\n",
    "    .take(num_2k_pos))\n",
    "\n",
    "unique_2k_neg_twograms = set(negative_twogram_counts \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .filter(lambda x: not x in common_2k_twograms) \\\n",
    "    .take(num_2k_neg))\n",
    "\n",
    "bag_5k_b = common_2k_twograms.union(unique_2k_neg_twograms).union(unique_2k_pos_twograms).union(bag_3k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "9f5c9127-0763-4fc8-9770-0c19094c117a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1265 common TwoGrams\n",
      "We will include 367 unique Positive TwoGrams\n",
      "We will include 368 unique Negative TwoGrams\n",
      "We will include 3000 OneGrams\n",
      "There are 5000 NGrams in total\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {len(common_2k_twograms)} common TwoGrams\")\n",
    "print(f\"We will include {len(unique_2k_pos_twograms)} unique Positive TwoGrams\")\n",
    "print(f\"We will include {len(unique_2k_neg_twograms)} unique Negative TwoGrams\")\n",
    "print(f\"We will include {len(bag_3k)} OneGrams\")\n",
    "print(f\"There are {len(bag_5k_b)} NGrams in total\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd4ed51-8b5d-497e-bed3-6ecb7775600e",
   "metadata": {},
   "source": [
    "## Create Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb6313d-009a-4fb4-bbd5-f080934b12a5",
   "metadata": {},
   "source": [
    "### Generate OneGram RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "140025ac-391c-48a7-8b01-99354e9d4d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "onegram_tuples_rdd = basic_rdd \\\n",
    "    .map(lambda x: (x[0], [tuple([s]) for s in x[1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae3cafe-488c-4258-ae57-54a2e07482cb",
   "metadata": {},
   "source": [
    "### Generate TwoGram RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "bd59ca2b-eb69-4bc0-8d85-f4ff0d1047e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "def one_and_two_grams(x):\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        result.append(tuple([s])) # Get all OneGrams\n",
    "    for i in range(len(x[1])-n+1):\n",
    "        gram = []\n",
    "        for j in range(i, i+n):\n",
    "            gram.append(x[1][j])\n",
    "        result.append(tuple(gram)) # Get all TwoGrams\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "867a83c4-e83a-4529-a1c1-c8b85eedb2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "twogram_tuples_rdd = basic_rdd.map(one_and_two_grams)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c223c1-a93d-4d0c-bc04-8a98d2f90f90",
   "metadata": {},
   "source": [
    "### Filter Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "f9c19b5d-5785-4ff6-ad78-a0c32491e7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_filter(x, bag):\n",
    "    global count\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if s in bag:\n",
    "            result.append(s)\n",
    "    return (x[0], result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7243cd86-9090-47d6-aa3c-d0a1d64ffec5",
   "metadata": {},
   "source": [
    "### Filter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "95c0569a-f4b9-414f-8c86-67c525886eea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "data_3k_rdd = onegram_tuples_rdd \\\n",
    "    .map(lambda x: bag_filter(x, bag_3k)) \\\n",
    "    .filter(lambda x: len(x[1]) > 0) # Ensure there is at least one feature\n",
    "\n",
    "data_5k_a_rdd = onegram_tuples_rdd \\\n",
    "    .map(lambda x: bag_filter(x, bag_5k_a)) \\\n",
    "    .filter(lambda x: len(x[1]) > 0) # Ensure there is at least one feature\n",
    "\n",
    "data_5k_b_rdd = twogram_tuples_rdd \\\n",
    "    .map(lambda x: bag_filter(x, bag_5k_b)) \\\n",
    "    .filter(lambda x: len(x[1]) > 0) # Ensure there is at least one feature\n",
    "\n",
    "num_3k_remaining = data_3k_rdd.count()\n",
    "num_5k_a_remaining = data_5k_a_rdd.count()\n",
    "num_5k_b_remaining = data_5k_b_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "37d1e155-d1b6-43d5-8db0-f83d8ab1d019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.32% of rows remain after Bag_3k filter\n",
      "99.50% of rows remain after Bag_5k_a filter\n",
      "99.32% of rows remain after Bag_5k_b filter\n"
     ]
    }
   ],
   "source": [
    "print(f\"{num_3k_remaining/total_rows*100:.2f}% of rows remain after Bag_3k filter\")\n",
    "print(f\"{num_5k_a_remaining/total_rows*100:.2f}% of rows remain after Bag_5k_a filter\")\n",
    "print(f\"{num_5k_b_remaining/total_rows*100:.2f}% of rows remain after Bag_5k_b filter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8bf80d2-3c3b-4f1f-b0a9-bf42db2168d8",
   "metadata": {},
   "source": [
    "## Prepare Data for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68dc175-df02-49ca-b28a-ac7347d60131",
   "metadata": {},
   "source": [
    "### Convert to RDD\\<LabeledPoint\\>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "7770e6d4-eede-4c68-a848-4069478f8b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.linalg import SparseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "id": "91fa812d-e55a-4377-af19-dfeca9917be8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_lp_rdd(x, bag):\n",
    "    feature_ids = []\n",
    "    feature_vals = []\n",
    "    for id,val in enumerate(bag):\n",
    "        if val in x[1]:\n",
    "            feature_ids.append(id)\n",
    "            feature_vals.append(1)\n",
    "    # return LabeledPoint(x[0], feature_\n",
    "    return LabeledPoint(x[0], SparseVector(len(bag), feature_ids, feature_vals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "91d7e834-5dcd-468e-a62d-051f8fb9bc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "5dc6ca5c-3d23-4b58-8bd3-b8e3e260920b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "model_A = data_3k_rdd.map(lambda x: convert_to_lp_rdd(x, bag_3k))\n",
    "model_B = data_5k_a_rdd.map(lambda x: convert_to_lp_rdd(x, bag_5k_a))\n",
    "model_C = data_5k_b_rdd.map(lambda x: convert_to_lp_rdd(x, bag_5k_b))\n",
    "\n",
    "if not os.path.isdir(\"data/model_A\"):\n",
    "    MLUtils.saveAsLibSVMFile(model_A, \"data/model_A\")\n",
    "if not os.path.isdir(\"data/model_B\"):\n",
    "    MLUtils.saveAsLibSVMFile(model_B, \"data/model_B\")\n",
    "if not os.path.isdir(\"data/model_C\"):\n",
    "    MLUtils.saveAsLibSVMFile(model_C, \"data/model_C\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
