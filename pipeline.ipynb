{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c72156a4-891d-4830-9f98-0a0626e756d4",
   "metadata": {},
   "source": [
    "# Amazon Review Data Pipeline\n",
    "\n",
    "The goal of this jupyter notebook is to:\n",
    "1. Load Amazon Review data acquired from [https://nijianmo.github.io/amazon/index.html](https://nijianmo.github.io/amazon/index.html)\n",
    "2. Turn it into a Spark RDD\n",
    "3. Create a Bag-of-Words with:\n",
    "    - Single words\n",
    "    - 1-grams and 2-grams\n",
    "4. Visualize the frequency of these words occurences in the Review data\n",
    "5. Create a LabeledPoint RDD for each review and save it as a libsvm file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a92afc-eb81-4451-84ad-3208159a4606",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing Data\n",
    "\n",
    "The data cannot be read directly from it's gzipped format into a Spark DataFrame. It produces Schema errors because there are a few naming collisions in the Json data.\n",
    "\n",
    "Instead, the data must be unzipped and a few of its errors must be correct before being loaded.\n",
    "\n",
    "The following script will achieve that. Specifically, it will ensure consistent casing for the field names 'style' and 'styleName'.\n",
    "\n",
    "Note: it is possible that both `gzip` and `sed` will need to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a35d6ebd-cbd5-4695-8b93-c259f87ab19c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "gzip: /home/steph/proj/550/project/data/Musical_Instruments_5.json already exists;\tnot overwritten\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "# Decompress Gzip\n",
    "gzip -dk $PWD/data/Musical_Instruments_5.json.gz\n",
    "\n",
    "# # Replace problematic field names\n",
    "sed -i \"s:style Name:styleName:\" $PWD/data/Musical_Instruments_5.json\n",
    "sed -i \"s:style name:styleName:\" $PWD/data/Musical_Instruments_5.json\n",
    "sed -i \"s:Style:style:\" $PWD/data/Musical_Instruments_5.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0d1b6a-9aec-4aab-8a23-a4a94b397263",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5faff0f9-acca-4a77-b5a9-762ed97e3fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import gzip\n",
    "import json\n",
    "import re\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a39253a-5156-4428-b534-86d2b1945682",
   "metadata": {},
   "source": [
    "## Spark DataFrame\n",
    "\n",
    "The goal of this section is to convert the Pandas dataframe from above into a Spark dataframe, to drop all columns except 'overall' (rating) and 'reviewText' and create an RDD from this data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0c803-8f60-45a2-810e-9f7cc0e8421d",
   "metadata": {},
   "source": [
    "#### Creating Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c0bdf37a-7078-40b3-974a-8eb339c9db01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/12/13 14:19:49 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[16]\") \\\n",
    "    .appName(\"data_pipeline\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f4ff55-f264-43e2-a753-eb3090b03cf4",
   "metadata": {},
   "source": [
    "#### Creating Spark Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0f1c2ab7-f511-4105-9de7-31417fbfc8c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- asin: string (nullable = true)\n",
      " |-- image: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      " |-- reviewTime: string (nullable = true)\n",
      " |-- reviewerID: string (nullable = true)\n",
      " |-- reviewerName: string (nullable = true)\n",
      " |-- style: struct (nullable = true)\n",
      " |    |-- Color Name:: string (nullable = true)\n",
      " |    |-- Color:: string (nullable = true)\n",
      " |    |-- Configuration:: string (nullable = true)\n",
      " |    |-- Edition:: string (nullable = true)\n",
      " |    |-- Format:: string (nullable = true)\n",
      " |    |-- Item Display Length:: string (nullable = true)\n",
      " |    |-- Item Package Quantity:: string (nullable = true)\n",
      " |    |-- Length:: string (nullable = true)\n",
      " |    |-- Model Number:: string (nullable = true)\n",
      " |    |-- Number of Items:: string (nullable = true)\n",
      " |    |-- Package Quantity:: string (nullable = true)\n",
      " |    |-- Package Type:: string (nullable = true)\n",
      " |    |-- Platform for Display:: string (nullable = true)\n",
      " |    |-- Platform:: string (nullable = true)\n",
      " |    |-- Product Packaging:: string (nullable = true)\n",
      " |    |-- Size Name:: string (nullable = true)\n",
      " |    |-- Size:: string (nullable = true)\n",
      " |    |-- style Name:: string (nullable = true)\n",
      " |    |-- style:: string (nullable = true)\n",
      " |    |-- styleName:: string (nullable = true)\n",
      " |-- summary: string (nullable = true)\n",
      " |-- unixReviewTime: long (nullable = true)\n",
      " |-- verified: boolean (nullable = true)\n",
      " |-- vote: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark_df = spark.read.json('data/Musical_Instruments_5.json')\n",
    "spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "80e3a67d-c472-44e2-9f99-a85852f8febc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 231392 rows of data\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {spark_df.count()} rows of data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c58fe8-6b1d-4b11-acf4-3e1a625355bf",
   "metadata": {},
   "source": [
    "## Cleaning Data\n",
    "\n",
    "1. Drop all columns except `overall` and `reviewTest`\n",
    "2. Drop all rows with NA values\n",
    "3. Convert to RDD\n",
    "4. Convert to lowercase\n",
    "5. Strip non-alphabetic/space chars\n",
    "6. Split into vector of strings\n",
    "7. Remove empty reviews\n",
    "8. Simplify rating into 0 or 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "796f6abc-37af-468f-94cc-8004a9f950ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231273"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop all columns except reviewText and overall\n",
    "df = spark_df[['overall', 'reviewText']]\n",
    "df.printSchema()\n",
    "\n",
    "# Drop all rows with NA values\n",
    "df = df.dropna()\n",
    "\n",
    "# Convert to rdd\n",
    "rdd = df.rdd\n",
    "\n",
    "# Convert to lowercase\n",
    "rdd = rdd.map(lambda x: (x[0], x[1].lower()))\n",
    "\n",
    "# Strip non-alphanumeric & space chars\n",
    "rdd = rdd.map(lambda x: (x[0], re.sub(r'[^A-Za-z ]+', '', x[1])))\n",
    "\n",
    "# Convert review to array of words\n",
    "rdd = rdd.map(lambda x: (x[0], x[1].split()))\n",
    "\n",
    "# Convert overall rating to 1 or 0\n",
    "# 4-5 -> Positive\n",
    "# 1-3 -> Negative\n",
    "rdd = rdd.map(lambda x: (1, x[1]) if x[0] in [4,5] else (0, x[1]))\n",
    "\n",
    "# Filter out empty reviews\n",
    "basic_rdd = rdd.filter(lambda x: len(x[1]) > 0)\n",
    "\n",
    "# Cache as basic_rdd\n",
    "basic_rdd.cache\n",
    "basic_rdd.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c1acde-6455-4afc-a215-e18839e971a8",
   "metadata": {},
   "source": [
    "## Create Bag-of-Words of 1-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08f52fd-e5a7-45f8-9b1f-68937f811e70",
   "metadata": {},
   "source": [
    "### Method for Filtering Stop Words\n",
    "\n",
    "Stopwords found [here](https://kavita-ganesan.com/what-are-stop-words/) and taken from [here](http://snowball.tartarus.org/algorithms/english/stop.txt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "7b49f4fb-4abb-47fa-b9b9-d28b0aef7af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "\n",
    "with open(\"data/stopwords.txt\") as file:\n",
    "    global stopwords\n",
    "    stopwords = file.read().splitlines()\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    global stopwords\n",
    "    result = []\n",
    "    for s in x[1]:\n",
    "        if not s in stopwords:\n",
    "            result.append(s)\n",
    "    return (x[0], result)\n",
    "    \n",
    "# def filter_stopwords(x):\n",
    "#     global stopwords\n",
    "#     result = []\n",
    "#     for s in x[1]:\n",
    "#         if isinstance(s, str):\n",
    "#             if not s in stopwords:\n",
    "#                 result.append(s)\n",
    "#             return (x[0], result)\n",
    "#         if not s[0] in stopwords or not s[-1] in stopwords:\n",
    "#             result.append(s)\n",
    "#     return (x[0], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "3e5e46c7-ed92-4db3-b70f-b2ac7650e74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onegram_flat_rdd = basic_rdd.map(remove_stopwords).flatMap(lambda x: x[1])\n",
    "onegram_rdd = onegram_flat_rdd.distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "02f9430d-31c6-418e-a2de-4a81bd3bbabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "word_counts = onegram_flat_rdd.map(lambda x: (x, 1)).reduceByKey(lambda a,b: a+b).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2739ebf6-bfb5-4a2f-9569-9a8945156510",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:========================>                                (7 + 9) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 156073 unique words in the corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"There are {onegram_rdd.count()} unique words in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "f765fd27-3a16-4fa4-b758-31b6a90cfd76",
   "metadata": {},
   "outputs": [],
   "source": [
    "onegram_counts_rdd = onegram_flat_rdd \\\n",
    "    .map(lambda x: (x, 1)) \\\n",
    "    .reduceByKey(lambda a,b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c70d5d56-d26c-4d48-9871-d46a36c91503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method RDD.cache of PythonRDD[218] at RDD at PythonRDD.scala:53>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onegram_counts_rdd.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "826edd75-6e63-48e3-b400-d3c5edc79cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 74:=================================================>      (14 + 2) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 93 shared words in the corpus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(f\"There are {onegram_shared_rdd.count()} shared words in the corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "3c5e3402-f1f5-4a1e-9073-e448f3a5460d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(-79048, 'great'),\n",
       " (-63701, 'good'),\n",
       " (-59495, 'guitar'),\n",
       " (-55327, 'sound'),\n",
       " (-54859, 'one'),\n",
       " (-53253, 'like'),\n",
       " (-49326, 'just'),\n",
       " (-48112, 'use'),\n",
       " (-43823, 'can'),\n",
       " (-43285, 'strings'),\n",
       " (-37369, 'will'),\n",
       " (-36488, 'get'),\n",
       " (-33601, 'price'),\n",
       " (-31183, 'really'),\n",
       " (-29998, 'quality'),\n",
       " (-29495, 'works'),\n",
       " (-29208, 'nice'),\n",
       " (-27165, 'little'),\n",
       " (-26956, 'dont'),\n",
       " (-25953, 'much')]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onegram_counts_rdd.filter(lambda x: x[1] > 10000).map(lambda x: (-x[1], x[0])).sortByKey().take(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6a9c0c-99ab-4d7a-bd4f-7e643b945973",
   "metadata": {},
   "source": [
    "## Convert to N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "07692f4e-043f-4414-9336-3baeedce42cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "231307"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_rdd.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "18729c13-492f-4a6d-a5b0-c8b104a335e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngram(n: int, x: [str]):\n",
    "    if n == 1:\n",
    "        return x\n",
    "    result = []\n",
    "    \n",
    "    for i in range(len(x)-n+1):\n",
    "        item = []\n",
    "        a = i\n",
    "        for a in range(i, i+n):\n",
    "            item.append(x[a])\n",
    "        result.append(item)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7fcaadbe-a17f-493f-a158-c205aae077ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num neg: 30768\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "22/12/13 15:09:26 ERROR Executor: Exception in task 1.0 in stage 24.0 (TID 229)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "TypeError: 'int' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 1.0 in stage 24.0 (TID 229) (172.17.16.130 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "TypeError: 'int' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "\n",
      "22/12/13 15:09:26 ERROR TaskSetManager: Task 1 in stage 24.0 failed 1 times; aborting job\n",
      "22/12/13 15:09:26 ERROR Executor: Exception in task 5.0 in stage 24.0 (TID 233)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "TypeError: 'int' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/13 15:09:26 ERROR Executor: Exception in task 8.0 in stage 24.0 (TID 236)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n",
      "    process()\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n",
      "    return f(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n",
      "    merger.mergeValues(iterator)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n",
      "    d[k] = comb(d[k], v) if k in d else creator(v)\n",
      "  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n",
      "    return f(*args, **kwargs)\n",
      "TypeError: 'int' object is not iterable\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n",
      "\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 2.0 in stage 24.0 (TID 230) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 0.0 in stage 24.0 (TID 228) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 15.0 in stage 24.0 (TID 243) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 7.0 in stage 24.0 (TID 235) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 6.0 in stage 24.0 (TID 234) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 13.0 in stage 24.0 (TID 241) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 3.0 in stage 24.0 (TID 231) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 9.0 in stage 24.0 (TID 237) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 4.0 in stage 24.0 (TID 232) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 14.0 in stage 24.0 (TID 242) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 10.0 in stage 24.0 (TID 238) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 11.0 in stage 24.0 (TID 239) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n",
      "22/12/13 15:09:26 WARN TaskSetManager: Lost task 12.0 in stage 24.0 (TID 240) (172.17.16.130 executor driver): TaskKilled (Stage cancelled)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 229) (172.17.16.130 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [72], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m shared_grams \u001b[38;5;241m=\u001b[39m ngram_counts\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# # count_total = ngrams_rdd.count()\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal ngrams:  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mngram_counts\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshared ngrams: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshared_grams\u001b[38;5;241m.\u001b[39mcount()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m shared_grams\u001b[38;5;241m.\u001b[39msortByKey()\u001b[38;5;241m.\u001b[39mcollect():\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1521\u001b[0m, in \u001b[0;36mRDD.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1514\u001b[0m \u001b[38;5;124;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1515\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1520\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1521\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1508\u001b[0m, in \u001b[0;36mRDD.sum\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[NumberOrArray]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumberOrArray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m   1500\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1501\u001b[0m \u001b[38;5;124;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[1;32m   1502\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1506\u001b[0m \u001b[38;5;124;03m    6.0\u001b[39;00m\n\u001b[1;32m   1507\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[1;32m   1509\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\n\u001b[1;32m   1510\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1336\u001b[0m, in \u001b[0;36mRDD.fold\u001b[0;34m(self, zeroValue, op)\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[1;32m   1333\u001b[0m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[1;32m   1334\u001b[0m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[1;32m   1335\u001b[0m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[0;32m-> 1336\u001b[0m vals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/rdd.py:1197\u001b[0m, in \u001b[0;36mRDD.collect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext):\n\u001b[1;32m   1196\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mctx\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1197\u001b[0m     sock_info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonRDD\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jrdd_deserializer))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/pyspark/sql/utils.py:190\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 190\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    192\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 24.0 failed 1 times, most recent failure: Lost task 1.0 in stage 24.0 (TID 229) (172.17.16.130 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2672)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2608)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2607)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2607)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1182)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1182)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2860)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2802)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2791)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:952)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2228)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2249)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2268)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2293)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1021)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:406)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1020)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 686, in main\n    process()\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 676, in process\n    out_iter = func(split_index, iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 3472, in pipeline_func\n    return func(split, prev_func(split, iterator))\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 540, in func\n    return f(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/rdd.py\", line 2554, in combineLocally\n    merger.mergeValues(iterator)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/shuffle.py\", line 255, in mergeValues\n    d[k] = comb(d[k], v) if k in d else creator(v)\n  File \"/home/steph/.local/lib/python3.8/site-packages/pyspark/util.py\", line 81, in wrapper\n    return f(*args, **kwargs)\nTypeError: 'int' object is not iterable\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:559)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:765)\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:747)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:512)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1211)\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:136)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:548)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1504)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:551)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\t... 1 more\n"
     ]
    }
   ],
   "source": [
    "ngram_rdd = basic_rdd.filter(lambda x: x[0] == 0)\n",
    "\n",
    "print(f\"Num neg: {ngram_rdd.count()}\")\n",
    "\n",
    "n = 4\n",
    "ngram_rdd = ngram_rdd.map(lambda x: (x[0], ngram(n, x[1]))).map(filter_stopwords).filter(lambda x: len(x[1]) > 0)\n",
    "\n",
    "# ngram_rdd_nosw = ngram_rdd.map(lambda x: (x[0], filter_stopwords(x[1])))\n",
    "# ngram_counts_rdd = ngram_rdd.map(lambda x: len(x[1]))\n",
    "# ngram_nosw_count_rdd = ngram_rdd_nosw.map(lambda x: len(x[1]))\n",
    "\n",
    "# def sum(a, b):\n",
    "#     return a+b\n",
    "\n",
    "if n == 1:\n",
    "    ngrams_rdd = ngram_rdd.flatMap(lambda x: x[1]).map(lambda x: (x, 1))\n",
    "else:\n",
    "    ngrams_rdd = ngram_rdd.flatMap(lambda x: x[1]).map(lambda x: (tuple(x), 1))\n",
    "ngram_counts = ngrams_rdd.reduceByKey(sum)\n",
    "\n",
    "print(type(ngram_counts))\n",
    "\n",
    "shared_grams = ngram_counts.filter(lambda x: x[1] > 100)\n",
    "\n",
    "# # count_total = ngrams_rdd.count()\n",
    "print(f\"total ngrams:  {ngram_counts.count()}\")\n",
    "print(f\"shared ngrams: {shared_grams.count()}\")\n",
    "\n",
    "for a in shared_grams.sortByKey().collect():\n",
    "    print(a)\n",
    "\n",
    "# ngrams_counts = ngrams_rdd.countByKey()\n",
    "\n",
    "# distinct_ngrams = ngrams_rdd.toDF().distinct()\n",
    "# print(f\"distinct ngrams: {distinct_ngrams.count()}\")\n",
    "\n",
    "# print(f\"No SW Filter:\\n\\tmin:  {ngram_counts_rdd.min()}\\n\\tmax:  {ngram_counts_rdd.max()}\\n\\tmean: {ngram_counts_rdd.mean()}\")\n",
    "# print(f\"With SW Filter:\\n\\tmin:  {ngram_nosw_count_rdd.min()}\\n\\tmax:  {ngram_nosw_count_rdd.max()}\\n\\tmean: {ngram_nosw_count_rdd.mean()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e1be018e-18d0-4c99-9c46-6766e899c058",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews w/ 1-grams: 230993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews w/ 2-grams: 222503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews w/ 3-grams: 209881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 222:===============================>                        (9 + 7) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reviews w/ 4-grams: 203566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "one_gram_rdd = basic_rdd.map(lambda x: (x[0], ngram(1, x[1]))).map(filter_stopwords).filter(lambda x: len(x[1]) > 0)\n",
    "two_gram_rdd = basic_rdd.map(lambda x: (x[0], ngram(2, x[1]))).map(filter_stopwords).filter(lambda x: len(x[1]) > 0)\n",
    "three_gram_rdd = basic_rdd.map(lambda x: (x[0], ngram(3, x[1]))).map(filter_stopwords).filter(lambda x: len(x[1]) > 0)\n",
    "four_gram_rdd = basic_rdd.map(lambda x: (x[0], ngram(4, x[1]))).map(filter_stopwords).filter(lambda x: len(x[1]) > 0)\n",
    "\n",
    "# print(\"Reviews w/ 1-grams: \" + str(one_gram_rdd.filter(lambda x: x[0] == 0).count()))\n",
    "# print(\"Reviews w/ 2-grams: \" + str(two_gram_rdd.filter(lambda x: x[0] == 0).count()))\n",
    "# print(\"Reviews w/ 3-grams: \" + str(three_gram_rdd.filter(lambda x: x[0] == 0).count()))\n",
    "# print(\"Reviews w/ 4-grams: \" + str(four_gram_rdd.filter(lambda x: x[0] == 0).count()))\n",
    "\n",
    "print(\"Reviews w/ 1-grams: \" + str(one_gram_rdd.count()))\n",
    "print(\"Reviews w/ 2-grams: \" + str(two_gram_rdd.count()))\n",
    "print(\"Reviews w/ 3-grams: \" + str(three_gram_rdd.count()))\n",
    "print(\"Reviews w/ 4-grams: \" + str(four_gram_rdd.count()))\n",
    "\n",
    "# for a in rdd.take(3):\n",
    "    # print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "2fc07dc0-eb05-4f26-8c56-f2be3aacaa47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157\n"
     ]
    }
   ],
   "source": [
    "filter_stopwords([\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "02081d3b-73af-4bba-a90f-f0723a49d937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4\n",
      "14\n",
      "12\n",
      "133\n",
      "4\n",
      "6\n",
      "62\n",
      "25\n",
      "19\n",
      "27\n",
      "4\n",
      "50\n",
      "4\n",
      "14\n",
      "12\n",
      "7\n",
      "3\n",
      "8838\n",
      "\n",
      "56\n",
      "10\n",
      "14\n",
      "6\n",
      "89\n",
      "33\n",
      "2\n",
      "29\n",
      "28\n",
      "7\n",
      "9\n",
      "8\n",
      "3173\n",
      "\n",
      "30\n",
      "37\n",
      "70\n",
      "23\n",
      "118\n",
      "8\n",
      "240\n",
      "105\n",
      "21\n",
      "23\n",
      "5\n",
      "20\n",
      "4\n",
      "4\n",
      "18\n",
      "46\n",
      "4\n",
      "8\n",
      "282\n",
      "32\n",
      "34\n",
      "90\n",
      "42\n",
      "98\n",
      "22\n",
      "56\n",
      "71\n",
      "250\n",
      "6\n",
      "7\n",
      "33\n",
      "14\n",
      "312\n",
      "83\n",
      "\n",
      "28\n",
      "70\n",
      "4\n",
      "22\n",
      "16\n",
      "2\n",
      "237\n",
      "29\n",
      "21\n",
      "10\n",
      "96\n",
      "20\n",
      "23\n",
      "94\n",
      "51\n",
      "84\n",
      "61\n",
      "81\n",
      "84\n",
      "2\n",
      "75\n",
      "38\n",
      "250\n",
      "114\n",
      "10\n",
      "7622\n",
      "\n",
      "2\n",
      "58\n",
      "2\n",
      "137\n",
      "25\n",
      "7\n",
      "30\n",
      "90\n",
      "5\n",
      "18\n",
      "88\n",
      "174\n",
      "\n",
      "29\n",
      "51\n",
      "4\n",
      "3\n",
      "78\n",
      "5\n",
      "9\n",
      "8\n",
      "6\n",
      "41\n",
      "1\n",
      "29\n",
      "3\n",
      "136\n",
      "2\n",
      "7\n",
      "46\n",
      "68\n",
      "25\n",
      "3\n",
      "105\n",
      "21\n",
      "46\n",
      "43\n",
      "182\n",
      "119\n",
      "49\n",
      "42\n",
      "40\n",
      "45\n",
      "67\n",
      "23\n",
      "146\n",
      "32\n",
      "38\n",
      "59\n",
      "14\n",
      "10\n",
      "16\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "None\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "rdd = convert_to_n_grams(2, basic_rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "39cce0ba-93db-4ea2-a0ba-3031c90cd4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.write.json(\"data/Musical_Instruments_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "705eb5a0-fc78-4f1a-b0d9-1e1376931de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.json(\"data/Musical_Instruments_2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e49ebb0-56d2-4ed3-9fb5-6516e0c6fce4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- overall: double (nullable = true)\n",
      " |-- reviewText: string (nullable = true)\n",
      "\n",
      "231344\n",
      "231344\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "print(df.count())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
